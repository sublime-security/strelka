#!/usr/bin/env python3
"""
strelka-backend

Command line utility for running Strelka backend server components.
"""
import argparse
import glob
import hashlib
import importlib
import io
import json
import logging.config
import math
import os
import re
import string
import sys
import time

import inflection
import interruptingcow
import magic
import redis
import yaml
import yara

from strelka import strelka, yara_extern


class Backend(object):
    def __init__(self, backend_cfg, coordinator):
        self.scanner_cache = {}
        self.backend_cfg = backend_cfg
        self.coordinator = coordinator
        self.limits = backend_cfg.get('limits')

        scanners = backend_cfg.get('scanners')
        if isinstance(scanners, str):
            logging.info(f'found scanners as string {scanners}')
            with open(scanners) as f:
                self.scanners = yaml.safe_load(f.read()).get('scanners')
        else:
            self.scanners = backend_cfg.get('scanners')

        self.compiled_magic = magic.Magic(
            magic_file=backend_cfg.get('tasting').get('mime_db'),
            mime=True,
        )

        yara_rules = backend_cfg.get('tasting').get('yara_rules')
        if os.path.isdir(yara_rules):
            yara_filepaths = {}
            globbed_yara = glob.iglob(
                f'{yara_rules}/**/*.yar*',
                recursive=True,
            )
            for (i, entry) in enumerate(globbed_yara):
                yara_filepaths[f'namespace{i}'] = entry
            self.compiled_yara = yara.compile(filepaths=yara_filepaths)
        else:
            self.compiled_yara = yara.compile(filepath=yara_rules)

    def work(self):
        logging.info('starting up')

        count = 0
        synced = 0

        work_start = time.time()
        work_expire = work_start + self.limits.get('time_to_live')

        while 1:
            if self.limits.get('max_files') != 0:
                if count >= self.limits.get('max_files'):
                    break
            if self.limits.get('time_to_live') != 0:
                if time.time() >= work_expire:
                    break

            task = self.coordinator.bzpopmin(['tasks', 'tasks_compile_yara', 'tasks_compile_and_sync_yara'], timeout=5)
            if task is None:
                continue

            (queue_name, root_id, expire_at) = task
            root_id = root_id.decode()
            expire_at = math.ceil(expire_at)
            timeout = math.ceil(expire_at - time.time())

            if timeout <= 0:
                continue

            if queue_name == b'tasks':
                file = strelka.File(pointer=root_id)

                try:
                    with interruptingcow.timeout(timeout,
                                                 strelka.RequestTimeout):
                        self.distribute(root_id, file, expire_at)
                        p = self.coordinator.pipeline(transaction=False)
                        p.rpush(f'event:{root_id}', 'FIN')
                        p.expireat(f'event:{root_id}', expire_at)
                        p.execute()

                except strelka.RequestTimeout:
                    logging.debug(f'request {root_id} timed out')
                except Exception:
                    logging.exception('unknown exception')

                count += 1

            elif queue_name == b'tasks_compile_yara':
                try:
                    with interruptingcow.timeout(timeout,
                                                 strelka.RequestTimeout):
                        errMsg = self.compile_yara(root_id)

                        if errMsg:
                            logging.error(errMsg)
                            self.coordinator.lpush(f'yara:compile:done:{root_id}', 'ERROR:' + errMsg)
                        else:
                            self.coordinator.lpush(f'yara:compile:done:{root_id}', 'FIN')

                except strelka.RequestTimeout:
                    logging.debug(f'request {root_id}:compile timed out')
                except Exception:
                    logging.exception('unknown exception')

            elif queue_name == b'tasks_compile_and_sync_yara':
                logging.info('syncing yara')
                try:
                    with interruptingcow.timeout(timeout,
                                                 strelka.RequestTimeout):
                        org_id = self.coordinator.get(f'org_id:{root_id}')
                        if not org_id:
                            continue
                        org_id = org_id.decode()
                        errMsg, nSynced = self.compile_and_sync_yara(org_id, root_id)
                        synced += nSynced
                        logging.info('synced:' + str(nSynced))

                        if errMsg:
                            self.coordinator.lpush(f'yara:compile_and_sync:done:{root_id}', 'ERROR:' + errMsg)
                        else:
                            self.coordinator.lpush(f'yara:compile_and_sync:done:{root_id}', 'FIN')

                except strelka.RequestTimeout:
                    logging.debug(f'request {root_id}:compile_and_sync timed out')
                except Exception:
                    logging.exception('unknown exception')

        logging.info(f'shutdown after scanning {count} file(s),'
                     f' syncing {synced} yara files, and'
                     f' {time.time() - work_start} second(s)')

    def compile_yara(self, root_id):
        data = b''
        errMsg = ''

        try:
            while 1:
                pop = self.coordinator.lpop(f'yara:compile:{root_id}')
                if not pop:
                    break
                data += pop

            try:
                yara.compile(source=data.decode(), externals=yara_extern.EXTERNAL_VARS)
            except (yara.Error, yara.SyntaxError) as e:
                errMsg = 'compiling yara: ' + str(e)
                logging.error(errMsg)
            except Exception as e2:
                errMsg = 'compiling yara: ' + str(e2)
                logging.error(errMsg)
        except Exception as e3:
            errMsg = 'retrieving yara: ' + str(e3)
            logging.error(errMsg)

        return errMsg

    def compile_and_sync_yara(self, org_id, root_id):
        synced = 0

        self.coordinator.delete(f'yara:compiled:{org_id}')
        self.coordinator.delete(f'yara:hash:{org_id}')

        hash = hashlib.sha256()
        errMsg = ''

        while 1:
            pop = self.coordinator.lpop(f'yara:compile_and_sync:{root_id}')
            if not pop:
                break

            data = {}
            try:
                data = json.loads(pop.decode())
            except Exception as e:
                errMsg = 'loading json: ' + str(e)
                logging.error(errMsg)
                continue

            try:
                compiled_yara = yara.compile(source=data['data'], externals=yara_extern.EXTERNAL_VARS)

                buf = io.BytesIO()
                compiled_yara.save(file=buf)

                self.coordinator.rpush(f'yara:compiled:{org_id}', buf.getvalue())

                hash.update(data['data'].encode())
                synced += 1
            except (yara.Error, yara.SyntaxError) as e:
                errMsg = 'compiling yara: ' + str(e)
                logging.error(errMsg)
            except Exception as e2:
                errMsg = 'compiling yara: ' + str(e2)
                logging.error(errMsg)

        self.coordinator.set(f'yara:hash:{org_id}', hash.hexdigest())
        self.coordinator.set(f'yara:synced:{root_id}', synced)

        return errMsg, synced

    def taste_mime(self, data):
        """Tastes file data with libmagic."""
        return [self.compiled_magic.from_buffer(data)]

    def taste_yara(self, data):
        """Tastes file data with YARA."""
        encoded_whitespace = string.whitespace.encode()
        stripped_data = data.lstrip(encoded_whitespace)
        yara_matches = self.compiled_yara.match(data=stripped_data)
        return [match.rule for match in yara_matches]

    def distribute(self, root_id, file, expire_at):
        """Distributes a file through scanners."""
        try:
            files = []

            try:
                with interruptingcow.timeout(self.limits.get('distribution'),
                                             exception=strelka.DistributionTimeout):
                    if file.depth > self.limits.get('max_depth'):
                        logging.info(f'request {root_id} exceeded maximum depth')
                        return

                    data = b''
                    legacy_yara_data = b''
                    while 1:
                        pop = self.coordinator.lpop(f'data:{file.pointer}')
                        if pop is None:
                            break
                        data += pop

                        # We use the root_id to locate custom yara for this document,
                        # since both the parent document and all child documents will
                        # take this path, and we wish to evaluate each against the
                        # same set of yara rules.
                        yaraData = self.coordinator.get(f'yara:{root_id}') # backcompat

                    file.add_flavors({'mime': self.taste_mime(data)})
                    file.add_flavors({'yara': self.taste_yara(data)})
                    flavors = (
                        file.flavors.get('external', [])
                        + file.flavors.get('mime', [])
                        + file.flavors.get('yara', [])
                    )

                    scanner_list = []
                    for name in self.scanners:
                        mappings = self.scanners.get(name, {})
                        assigned = self.assign_scanner(
                            name,
                            mappings,
                            flavors,
                            file,
                        )
                        if assigned is not None:
                            scanner_list.append(assigned)
                    scanner_list.sort(
                        key=lambda k: k.get('priority', 5),
                        reverse=True,
                    )

                    p = self.coordinator.pipeline(transaction=False)
                    tree_dict = {
                        'node': file.uid,
                        'parent': file.parent,
                        'root': root_id,
                    }

                    if file.depth == 0:
                        tree_dict['node'] = root_id
                    if file.depth == 1:
                        tree_dict['parent'] = root_id

                    file_dict = {
                        'depth': file.depth,
                        'name': file.name,
                        'flavors': file.flavors,
                        'scanners': [s.get('name') for s in scanner_list],
                        'size': len(data),
                        'source': file.source,
                        'tree': tree_dict,
                    }
                    scan = {}

                    for scanner in scanner_list:
                        try:
                            name = scanner['name']
                            options = scanner.get('options', {})

                            if name == 'ScanYara':
                                logging.info('scanning')
                                compiled_custom_yara = []

                                org_id = self.coordinator.get(f'org_id:{root_id}')
                                if not org_id:
                                    continue

                                org_id = org_id.decode()
                                yaraItems = self.coordinator.lrange(f'yara:compiled:{org_id}', 0, -1)
                                for yaraData in yaraItems:
                                    if not yaraData:
                                        break
                                    buf = io.BytesIO(yaraData)
                                    yaraDataC = yara.load(file=buf)
                                    compiled_custom_yara.append(yaraDataC)
                                options['compiled_custom_yara'] = compiled_custom_yara

                                if legacy_yara_data: # backcompat
                                    options['source'] = legacy_yara_data.decode()

                            und_name = inflection.underscore(name)
                            scanner_import = f'strelka.scanners.{und_name}'
                            module = importlib.import_module(scanner_import)
                            if und_name not in self.scanner_cache:
                                attr = getattr(module, name)(self.backend_cfg, self.coordinator)
                                self.scanner_cache[und_name] = attr
                            plugin = self.scanner_cache[und_name]
                            (f, s) = plugin.scan_wrapper(
                                data,
                                file,
                                options,
                                expire_at,
                            )
                            files.extend(f)

                            scan = {
                                **scan,
                                **s,
                            }

                        except ModuleNotFoundError:
                            logging.exception(f'scanner {name} not found')

                        except Exception as e:
                            logging.exception('error: ' + str(e))

                    event = {
                        **{'file': file_dict},
                        **{'scan': scan},
                    }

                    p.rpush(f'event:{root_id}', strelka.format_event(event))
                    p.expireat(f'event:{root_id}', expire_at)
                    p.execute()

            except strelka.DistributionTimeout:
                logging.exception(f'node {file.uid} timed out')

            for f in files:
                f.parent = file.uid
                f.depth = file.depth + 1
                self.distribute(root_id, f, expire_at)

        except strelka.RequestTimeout:
            raise

    def assign_scanner(self, scanner, mappings, flavors, file):
        """Assigns scanners based on mappings and file data.

        Performs the task of assigning scanners based on the scan configuration
        mappings and file flavors, filename, and source. Assignment supports
        positive and negative matching: scanners are assigned if any positive
        categories are matched and no negative categories are matched. Flavors are
        literal matches, filename and source matches uses regular expressions.

        Args:
            scanner: Name of the scanner to be assigned.
            mappings: List of dictionaries that contain values used to assign
                the scanner.
            flavors: List of file flavors to use during scanner assignment.
            filename: Filename to use during scanner assignment.
            source: File source to use during scanner assignment.
        Returns:
            Dictionary containing the assigned scanner or None.
        """
        for mapping in mappings:
            negatives = mapping.get('negative', {})
            positives = mapping.get('positive', {})
            neg_flavors = negatives.get('flavors', [])
            neg_filename = negatives.get('filename', None)
            neg_source = negatives.get('source', None)
            pos_flavors = positives.get('flavors', [])
            pos_filename = positives.get('filename', None)
            pos_source = positives.get('source', None)
            assigned = {'name': scanner,
                        'priority': mapping.get('priority', 5),
                        'options': mapping.get('options', {})}

            for neg_flavor in neg_flavors:
                if neg_flavor in flavors:
                    return None
            if neg_filename is not None:
                if re.search(neg_filename, file.name) is not None:
                    return None
            if neg_source is not None:
                if re.search(neg_source, file.source) is not None:
                    return None
            for pos_flavor in pos_flavors:
                if pos_flavor == '*' or pos_flavor in flavors:
                    return assigned
            if pos_filename is not None:
                if re.search(pos_filename, file.name) is not None:
                    return assigned
            if pos_source is not None:
                if re.search(pos_source, file.source) is not None:
                    return assigned
        return None


def main():
    parser = argparse.ArgumentParser(prog='strelka-worker',
                                     description='runs Strelka workers',
                                     usage='%(prog)s [options]')
    parser.add_argument('-c', '--worker-config',
                        action='store',
                        dest='backend_cfg_path',
                        help='path to server configuration file')
    args = parser.parse_args()

    backend_cfg_path = ''
    if args.backend_cfg_path:
        if not os.path.exists(args.backend_cfg_path):
            logging.exception(f'backend configuration {args.backend_cfg_path} does not exist')
            sys.exit()
        backend_cfg_path = args.backend_cfg_path
    elif os.path.exists('/etc/strelka/backend.yaml'):
        backend_cfg_path = '/etc/strelka/backend.yaml'
    else:
        logging.exception('no backend configuration found')
        sys.exit()

    with open(backend_cfg_path) as f:
        backend_cfg = yaml.safe_load(f.read())

    log_cfg_path = backend_cfg.get('logging_cfg')
    with open(log_cfg_path) as f:
        logging.config.dictConfig(yaml.safe_load(f.read()))
    logging.info(f'using backend configuration {backend_cfg_path}')

    try:
        coordinator_cfg = backend_cfg.get('coordinator')
        coordinator_addr = coordinator_cfg.get('addr').split(':')
        coordinator = redis.StrictRedis(
            host=coordinator_addr[0],
            port=coordinator_addr[1],
            db=coordinator_cfg.get('db'),
        )
        if coordinator.ping():
            logging.debug('verified coordinator is up')

    except Exception:
        logging.exception('coordinator unavailable')
        sys.exit()

    backend = Backend(backend_cfg, coordinator)
    backend.work()


if __name__ == '__main__':
    main()
