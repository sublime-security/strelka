#!/usr/bin/env python3
"""
strelka-backend

Command line utility for running Strelka backend server components.
"""
import argparse
from datetime import datetime
import glob
import hashlib
import importlib
import io
import json
import logging.config
import math
import os
import re
import string
import sys
import time
import signal
import threading

import inflection
import interruptingcow
import magic
import redis
import yaml
import yara

from strelka import strelka, yara_extern
from pythonjsonlogger.json import JsonFormatter

shutdown_event = threading.Event()

def enable_json_logging():
    # Get root logger
    logger = logging.getLogger()

    # Log to stdout (it logs to stderr by default)
    handler = logging.StreamHandler(stream=sys.stdout)

    # Set up the JSON formatter for the handler
    formatter = JsonFormatter(
        '%(asctime)s %(levelname)s %(message)s',
        datefmt='%Y-%m-%dT%H:%M:%S%z'
    )
    handler.setFormatter(formatter)

    # Override the default handlers and attach our JSON handler
    logger.handlers = []
    logger.addHandler(handler)


def trace(msg, extra=None):
    if 'ENABLE_TRACE_LOGGING' in os.environ:
        logging.info(f'[TRACE] {msg}', extra=extra)


def trace_scanner(scanner, msg, extra=None):
    if 'ENABLE_SCANNER_TRACE_LOGGING' in os.environ:
        logging.info(f'[TRACE][SCANNER] {msg}', extra={
            'scanner': scanner,
            **extra
        })


class Backend(object):

    def __init__(self, backend_cfg, coordinator):
        self.scanner_cache = {}
        self.backend_cfg = backend_cfg
        self.coordinator = coordinator
        self.limits = backend_cfg.get('limits')

        scanners = backend_cfg.get('scanners')
        if isinstance(scanners, str):
            logging.info(f'found scanners as string {scanners}')
            with open(scanners) as f:
                self.scanners = yaml.safe_load(f.read()).get('scanners')
        else:
            self.scanners = backend_cfg.get('scanners')

        self.compiled_magic = magic.Magic(
            magic_file=backend_cfg.get('tasting').get('mime_db'),
            mime=True,
        )

        yara_rules = backend_cfg.get('tasting').get('yara_rules')
        if os.path.isdir(yara_rules):
            yara_filepaths = {}
            globbed_yara = glob.iglob(
                f'{yara_rules}/**/*.yar*',
                recursive=True,
            )
            for (i, entry) in enumerate(globbed_yara):
                yara_filepaths[f'namespace{i}'] = entry
            self.compiled_yara = yara.compile(filepaths=yara_filepaths)
        else:
            self.compiled_yara = yara.compile(filepath=yara_rules)

    def work(self):
        logging.info('starting up')

        count = 0
        synced = 0

        work_start = time.time()
        work_expire = work_start + self.limits.get('time_to_live')

        while not shutdown_event.is_set():
            if self.limits.get('max_files') != 0:
                if count >= self.limits.get('max_files'):
                    break
            if self.limits.get('time_to_live') != 0:
                if time.time() >= work_expire:
                    break

            start_pop_time = datetime.now()
            task = self.coordinator.bzpopmin(['tasks', 'tasks_compile_yara', 'tasks_compile_and_sync_yara'], timeout=5)
            end_pop_time = datetime.now()
            receive_time_ms = (end_pop_time - start_pop_time).total_seconds() * 1000
            if task is None:
                trace('received no tasks', extra={
                    'receive_time_ms': receive_time_ms
                })
                continue

            (queue_name, root_id, expire_at) = task
            root_id = root_id.decode()
            expire_at = math.ceil(expire_at)
            timeout = math.ceil(expire_at - time.time())

            if timeout <= 0:
                trace('received expired task', extra={
                    'strelka_id': root_id,
                    'deadline': expire_at,
                    'receive_time_ms': receive_time_ms
                })
                continue

            if shutdown_event.is_set():
                trace(f'Received task after shutdown signal, re-queuing {task}.', extra={
                    'strelka_id': root_id
                })
                # We picked up a task after shutdown_event was set. We'll put it back on the queue for another worker
                self.coordinator.zadd(queue_name, {root_id: expire_at})
                break

            if queue_name == b'tasks':
                trace('received scan file task', extra={
                    'strelka_id': root_id,
                    'deadline': expire_at,
                    'receive_time_ms': receive_time_ms
                })
                file = strelka.File(pointer=root_id)

                try:
                    with interruptingcow.timeout(timeout,
                                                 strelka.RequestTimeout):
                        start_scan_time = datetime.now()
                        files_scanned = self.distribute(root_id, file, expire_at)
                        end_scan_time = datetime.now()
                        trace('full scan complete', extra={
                            'strelka_id': root_id,
                            'deadline': expire_at,
                            'files_scanned': files_scanned,
                            'full_scan_took_ms': (end_scan_time - start_scan_time).total_seconds() * 1000
                        })

                        start_send_fin_time = datetime.now()
                        p = self.coordinator.pipeline(transaction=False)
                        p.rpush(f'event:{root_id}', 'FIN')
                        p.expireat(f'event:{root_id}', expire_at)
                        p.execute()
                        end_send_fin_time = datetime.now()
                        trace('FIN event emitted', extra={
                            'strelka_id': root_id,
                            'deadline': expire_at,
                            'fin_emit_took_ms': (end_send_fin_time - start_send_fin_time).total_seconds() * 1000
                        })

                except strelka.RequestTimeout:
                    trace('scan timed out', extra={
                        'strelka_id': root_id
                    })
                except Exception as e:
                    trace('scan encountered an error', extra={
                        'strelka_id': root_id,
                        'error': str(e)
                    })

                count += 1

            elif queue_name == b'tasks_compile_yara':
                try:
                    with interruptingcow.timeout(timeout,
                                                 strelka.RequestTimeout):
                        errMsg = self.compile_yara(root_id)

                        if errMsg:
                            logging.error(errMsg)
                            self.coordinator.lpush(f'yara:compile:done:{root_id}', 'ERROR:' + errMsg)
                        else:
                            self.coordinator.lpush(f'yara:compile:done:{root_id}', 'FIN')

                except strelka.RequestTimeout:
                    logging.debug(f'request {root_id}:compile timed out')
                except Exception:
                    logging.exception('unknown exception')

            elif queue_name == b'tasks_compile_and_sync_yara':
                logging.info('syncing yara')
                try:
                    with interruptingcow.timeout(timeout,
                                                 strelka.RequestTimeout):
                        yara_cache_key = self.coordinator.get(f'yara_cache_key:{root_id}')
                        if not yara_cache_key:
                            continue
                        yara_cache_key = yara_cache_key.decode()
                        errMsg, nSynced = self.compile_and_sync_yara(yara_cache_key, root_id)
                        synced += nSynced
                        logging.info('synced:' + str(nSynced))

                        if errMsg:
                            self.coordinator.lpush(f'yara:compile_and_sync:done:{root_id}', 'ERROR:' + errMsg)
                        else:
                            self.coordinator.lpush(f'yara:compile_and_sync:done:{root_id}', 'FIN')

                except strelka.RequestTimeout:
                    logging.debug(f'request {root_id}:compile_and_sync timed out')
                except Exception:
                    logging.exception('unknown exception')

        logging.info(f'shutdown after scanning {count} file(s),'
                     f' syncing {synced} yara files, and'
                     f' {time.time() - work_start} second(s)'
                     f' should shutdown trigger: {shutdown_event.is_set()}')

    def compile_yara(self, root_id):
        data = b''
        errMsg = ''

        try:
            while 1:
                pop = self.coordinator.lpop(f'yara:compile:{root_id}')
                if not pop:
                    break
                data += pop

            try:
                yara.compile(source=data.decode(), externals=yara_extern.EXTERNAL_VARS)
            except (yara.Error, yara.SyntaxError) as e:
                errMsg = 'compiling yara: ' + str(e)
                logging.error(errMsg)
            except Exception as e2:
                errMsg = 'compiling yara: ' + str(e2)
                logging.error(errMsg)
        except Exception as e3:
            errMsg = 'retrieving yara: ' + str(e3)
            logging.error(errMsg)

        return errMsg

    def compile_and_sync_yara(self, yara_cache_key, root_id):
        synced = 0

        self.coordinator.delete(f'yara:compiled_all:{yara_cache_key}')
        self.coordinator.delete(f'yara:hash:{yara_cache_key}')

        hash = hashlib.sha256()
        errMsg = ''
        yara_src = ''

        while 1:
            pop = self.coordinator.lpop(f'yara:compile_and_sync:{root_id}')
            if not pop:
                break

            data = {}
            try:
                data = json.loads(pop.decode())
            except Exception as e:
                errMsg = 'loading json: ' + str(e)
                logging.error(errMsg)
                continue

            try:
                # compile single signature for validation
                compiled_yara = yara.compile(source=data['data'], externals=yara_extern.EXTERNAL_VARS)

                # append to source if compilation succeeds
                yara_src += data['data']
                synced += 1
            except (yara.Error, yara.SyntaxError) as e:
                errMsg = 'compiling yara: ' + str(e)
                logging.error(errMsg)
            except Exception as e2:
                errMsg = 'compiling yara: ' + str(e2)
                logging.error(errMsg)

        if yara_src:
            # compile all valid signatures into single object for faster execution
            try:
                compiled_yara = yara.compile(source=yara_src, externals=yara_extern.EXTERNAL_VARS)
                buf = io.BytesIO()
                compiled_yara.save(file=buf)

                hash.update(yara_src.encode())

                self.coordinator.set(f'yara:compiled_all:{yara_cache_key}', buf.getvalue())
            except (yara.Error, yara.SyntaxError) as e:
                errMsg = 'compiling yara: ' + str(e)
                logging.error(errMsg)
                synced = 0
            except Exception as e2:
                errMsg = 'compiling yara: ' + str(e2)
                logging.error(errMsg)
                synced = 0

        self.coordinator.set(f'yara:hash:{yara_cache_key}', hash.hexdigest())
        self.coordinator.set(f'yara:synced:{root_id}', synced)

        return errMsg, synced

    def taste_mime(self, data):
        """Tastes file data with libmagic."""
        return [self.compiled_magic.from_buffer(data)]

    def taste_yara(self, data):
        """Tastes file data with YARA."""
        encoded_whitespace = string.whitespace.encode()
        stripped_data = data.lstrip(encoded_whitespace)
        yara_matches = self.compiled_yara.match(data=stripped_data)
        return [match.rule for match in yara_matches]

    def distribute(self, root_id, file, expire_at):
        """Distributes a file through scanners."""
        try:
            files = []

            try:
                with interruptingcow.timeout(self.limits.get('distribution'),
                                             exception=strelka.DistributionTimeout):
                    start_file_scan_time = datetime.now()
                    if file.depth > self.limits.get('max_depth'):
                        logging.info(f'request {root_id} exceeded maximum depth', extra={
                            'strelka_id': root_id
                        })
                        return 0

                    data = b''
                    legacy_yara_data = b''

                    start_pop_data_time = datetime.now()
                    while 1:
                        pop = self.coordinator.lpop(f'data:{file.pointer}')
                        if pop is None:
                            break
                        data += pop

                        # We use the root_id to locate custom yara for this document,
                        # since both the parent document and all child documents will
                        # take this path, and we wish to evaluate each against the
                        # same set of yara rules.
                        legacy_yara_data = self.coordinator.get(f'yara:{root_id}') # backcompat
                    end_pop_data_time = datetime.now()

                    start_taste_time = datetime.now()
                    file.add_flavors({'mime': self.taste_mime(data)})
                    file.add_flavors({'yara': self.taste_yara(data)})
                    flavors = (
                        file.flavors.get('external', [])
                        + file.flavors.get('mime', [])
                        + file.flavors.get('yara', [])
                    )
                    end_taste_time = datetime.now()

                    scanner_list = []
                    for name in self.scanners:
                        mappings = self.scanners.get(name, {})
                        assigned = self.assign_scanner(
                            name,
                            mappings,
                            flavors,
                            file,
                        )
                        if assigned is not None:
                            scanner_list.append(assigned)
                    scanner_list.sort(
                        key=lambda k: k.get('priority', 5),
                        reverse=True,
                    )

                    p = self.coordinator.pipeline(transaction=False)
                    tree_dict = {
                        'node': file.uid,
                        'parent': file.parent,
                        'root': root_id,
                    }

                    if file.depth == 0:
                        tree_dict['node'] = root_id
                    if file.depth == 1:
                        tree_dict['parent'] = root_id

                    file_dict = {
                        'depth': file.depth,
                        'name': file.name,
                        'flavors': file.flavors,
                        'scanners': [s.get('name') for s in scanner_list],
                        'size': len(data),
                        'source': file.source,
                        'tree': tree_dict,
                    }
                    scan = {}

                    for scanner in scanner_list:
                        name = scanner['name']
                        try:
                            options = scanner.get('options', {})
                            options['strelka_id'] = root_id
                            if name == 'ScanYara':
                                start_yara_retrieval = datetime.now()
                                yara_load_took_ms = 0
                                yara_cache_key = self.coordinator.get(f'yara_cache_key:{root_id}')
                                if yara_cache_key:
                                    yara_cache_key = yara_cache_key.decode()
                                    yara_data = self.coordinator.get(f'yara:compiled_all:{yara_cache_key}')
                                    if yara_data:
                                        start_yara_load = datetime.now()
                                        buf = io.BytesIO(yara_data)
                                        yara_dataC = yara.load(file=buf)
                                        options['compiled_custom_yara_all'] = yara_dataC
                                        end_yara_load = datetime.now()
                                        yara_load_took_ms = (end_yara_load - start_yara_load).total_seconds() * 1000

                                if legacy_yara_data: # backcompat
                                    options['source'] = legacy_yara_data.decode()

                                end_yara_retrieval = datetime.now()
                                trace_scanner(name, 'yara data retrieved', extra={
                                    'strelka_id': root_id,
                                    'deadline': expire_at,
                                    'yara_retrieval_took_ms': (end_yara_retrieval - start_yara_retrieval).total_seconds() * 1000,
                                    'yara_load_took_ms': yara_load_took_ms
                                })

                            start_scanner_time = datetime.now()
                            und_name = inflection.underscore(name)
                            scanner_import = f'strelka.scanners.{und_name}'
                            module = importlib.import_module(scanner_import)
                            if und_name not in self.scanner_cache:
                                attr = getattr(module, name)(self.backend_cfg, self.coordinator)
                                self.scanner_cache[und_name] = attr
                            plugin = self.scanner_cache[und_name]
                            (f, s) = plugin.scan_wrapper(
                                data,
                                file,
                                options,
                                expire_at,
                            )
                            files.extend(f)

                            scan = {
                                **scan,
                                **s,
                            }
                            end_scanner_time = datetime.now()

                            scanner_limit_secs = self.limits.get('scanner')
                            runaway_scanner = False
                            if scanner_limit_secs:
                                scanner_took_secs = (end_scanner_time - start_scanner_time).total_seconds()
                                runaway_scanner = (scanner_limit_secs - scanner_took_secs) <= 0

                            trace_scanner(name, 'scan completed', extra={
                                'strelka_id': root_id,
                                'deadline': expire_at,
                                'scanner_took_ms': scanner_took_secs * 1000,
                                'runaway_scanner': runaway_scanner
                            })

                        except ModuleNotFoundError:
                            trace_scanner(name, 'scanner not found', extra={
                                'strelka_id': root_id,
                                'deadline': expire_at,
                            })
                        except strelka.RequestTimeout:
                            trace_scanner(name, 'scanner timed out', extra={
                                'strelka_id': root_id,
                                'deadline': expire_at
                            })
                        except Exception as e:
                            trace_scanner(name, 'scanner encountered an error', extra={
                                'strelka_id': root_id,
                                'deadline': expire_at,
                                'error': str(e)
                            })

                    event = {
                        **{'file': file_dict},
                        **{'scan': scan},
                        **{'backend': {'release_version': os.environ.get('RELEASE_VERSION', '')}},
                    }
                    end_scan_file_time = datetime.now()

                    start_emit_result_time = datetime.now()
                    p.rpush(f'event:{root_id}', strelka.format_event(event))
                    p.expireat(f'event:{root_id}', expire_at)
                    p.execute()
                    end_emit_result_time = datetime.now()
                    trace('file scan complete', extra={
                        'strelka_id': root_id,
                        'deadline': expire_at,
                        'scanner_count': len(scanner_list),
                        'tasting_took_ms': (end_taste_time - start_taste_time).total_seconds() * 1000,
                        'data_collection_took_ms': (end_pop_data_time - start_pop_data_time).total_seconds() * 1000,
                        'file_scan_took_ms': (end_scan_file_time - start_file_scan_time).total_seconds() * 1000,
                        'result_emit_took_ms': (end_emit_result_time - start_emit_result_time).total_seconds() * 1000
                    })

            except strelka.DistributionTimeout:
                trace('file scan timed out', extra={
                    'strelka_id': root_id
                })

            nested_file_counts = 0
            for f in files:
                f.parent = file.uid
                f.depth = file.depth + 1
                nested_file_counts += self.distribute(root_id, f, expire_at)

            return 1 + len(files) + nested_file_counts

        except strelka.RequestTimeout:
            raise

    def assign_scanner(self, scanner, mappings, flavors, file):
        """Assigns scanners based on mappings and file data.

        Performs the task of assigning scanners based on the scan configuration
        mappings and file flavors, filename, and source. Assignment supports
        positive and negative matching: scanners are assigned if any positive
        categories are matched and no negative categories are matched. Flavors are
        literal matches, filename and source matches uses regular expressions.

        Args:
            scanner: Name of the scanner to be assigned.
            mappings: List of dictionaries that contain values used to assign
                the scanner.
            flavors: List of file flavors to use during scanner assignment.
            filename: Filename to use during scanner assignment.
            source: File source to use during scanner assignment.
        Returns:
            Dictionary containing the assigned scanner or None.
        """
        for mapping in mappings:
            negatives = mapping.get('negative', {})
            positives = mapping.get('positive', {})
            neg_flavors = negatives.get('flavors', [])
            neg_filename = negatives.get('filename', None)
            neg_source = negatives.get('source', [])
            pos_flavors = positives.get('flavors', [])
            pos_filename = positives.get('filename', None)
            pos_source = positives.get('source', [])
            assigned = {'name': scanner,
                        'priority': mapping.get('priority', 5),
                        'options': mapping.get('options', {})}

            for neg_flavor in neg_flavors:
                if neg_flavor in flavors:
                    return None
            if neg_filename:
                if re.search(neg_filename, file.name):
                    return None
            if neg_source:
                if file.source in neg_source:
                    return None
            for pos_flavor in pos_flavors:
                if pos_flavor == '*' or pos_flavor in flavors:
                    return assigned
            if pos_filename:
                if re.search(pos_filename, file.name):
                    return assigned
            if pos_source:
                if file.source in pos_source:
                    return assigned
        return None

def handle_sigint(signum, frame):
    logging.info('Received SIGINT. Will attempt to finish any current tasks before shutting down.')
    shutdown_event.set()


def main():
    signal.signal(signal.SIGINT, handle_sigint)

    parser = argparse.ArgumentParser(prog='strelka-worker',
                                     description='runs Strelka workers',
                                     usage='%(prog)s [options]')
    parser.add_argument('-c', '--worker-config',
                        action='store',
                        dest='backend_cfg_path',
                        help='path to server configuration file')
    args = parser.parse_args()

    backend_cfg_path = ''
    if args.backend_cfg_path:
        if not os.path.exists(args.backend_cfg_path):
            logging.exception(f'backend configuration {args.backend_cfg_path} does not exist')
            sys.exit()
        backend_cfg_path = args.backend_cfg_path
    elif os.path.exists('/etc/strelka/backend.yaml'):
        backend_cfg_path = '/etc/strelka/backend.yaml'
    else:
        logging.exception('no backend configuration found')
        sys.exit()

    with open(backend_cfg_path) as f:
        backend_cfg = yaml.safe_load(f.read())

    log_cfg_path = backend_cfg.get('logging_cfg')
    with open(log_cfg_path) as f:
        logging.config.dictConfig(yaml.safe_load(f.read()))
    if 'ENABLE_JSON_LOGGING' in os.environ:
        enable_json_logging()

    logging.info(f'using backend configuration {backend_cfg_path}')

    try:
        coordinator_cfg = backend_cfg.get('coordinator')
        coordinator_addr = coordinator_cfg.get('addr').split(':')
        coordinator = redis.StrictRedis(
            host=coordinator_addr[0],
            port=coordinator_addr[1],
            db=coordinator_cfg.get('db'),
        )
        if coordinator.ping():
            logging.debug('verified coordinator is up')

    except Exception:
        logging.exception('coordinator unavailable')
        sys.exit()

    backend = Backend(backend_cfg, coordinator)
    backend.work()


if __name__ == '__main__':
    main()
